{
  "beginner": "## File I/O Operations for Beginners\n\n### Why File I/O Matters\n\nFiles are how programs store data permanently. Without file operations, everything your program does disappears when it closes. Learning to read and write files is essential for:\n- Saving user data (games, apps, forms)\n- Loading configuration settings\n- Processing data from external sources\n- Creating backups and logs\n\n### Core Concept: The File Object\n\nWhen you open a file in Python, you get a **file object**—a bridge between your code and the actual file on disk. Think of it like opening a book: you get a bookmark that tracks where you are.\n\n```python\n# Opening a file creates a file object\nfile = open('example.txt', 'r')\nprint(type(file))  # <class '_io.TextIOWrapper'>\nfile.close()  # Always close when done!\n```\n\n### Topic 1: Opening & Closing Files\n\nFiles must be opened before you can use them, and closed when finished.\n\n#### The `open()` Function\n\n```python\nopen(filename, mode='r', encoding='utf-8')\n```\n\n**Parameters:**\n- `filename`: Path to your file (string)\n- `mode`: How to open it (default is 'r' for read)\n- `encoding`: Text encoding (default is system-dependent, use 'utf-8')\n\n#### File Opening Modes\n\n| Mode | Name | Purpose | Overwrites? |\n|------|------|---------|-------------|\n| `'r'` | Read | Read existing file | No |\n| `'w'` | Write | Create/overwrite file | Yes! |\n| `'a'` | Append | Add to end of file | No |\n| `'x'` | Exclusive | Create only (error if exists) | Never |\n| `'r+'` | Read & Write | Both operations allowed | No |\n\n**Why this matters:** Using `'w'` accidentally will DELETE existing content. Use `'a'` if you want to keep what's there.\n\n```python\n# Reading (safe if file doesn't exist)\nfile = open('story.txt', 'r')  # Error if file doesn't exist!\nfile.close()\n\n# Writing (creates file or overwrites)\nfile = open('log.txt', 'w')  # Deletes old content if file existed\nfile.close()\n\n# Appending (adds to end)\nfile = open('log.txt', 'a')  # Keeps existing content, adds at end\nfile.close()\n\n# Exclusive (only creates if new)\nfile = open('new_file.txt', 'x')  # Error if file already exists\nfile.close()\n```\n\n#### Why Close Files?\n\nAlways close files when done:\n\n```python\nfile = open('data.txt', 'r')\ndata = file.read()\nfile.close()  # CRITICAL! Frees memory, saves changes, allows others to use file\n```\n\n**Common mistake:** Forgetting to close creates memory leaks and file locks:\n\n```python\n# DON'T DO THIS - file stays open\nfile = open('data.txt', 'r')\ndata = file.read()\nprint(data)  # File still open! Memory wasted!\n\n# DO THIS - file is closed\nfile = open('data.txt', 'r')\ndata = file.read()\nfile.close()  # Now properly cleaned up\nprint(data)\n```\n\n### Topic 2: The `with` Statement (Best Practice)\n\nThe `with` statement automatically closes your file, even if errors occur. This is the modern, recommended way.\n\n```python\n# with statement - automatic closing (BEST PRACTICE)\nwith open('example.txt', 'r') as file:\n    data = file.read()\n    print(data)\n# File automatically closed here!\n\n# This is equivalent to:\nfile = open('example.txt', 'r')\ntry:\n    data = file.read()\n    print(data)\nfinally:\n    file.close()  # Guaranteed to run\n```\n\n**Why this matters:** Even if your code crashes, the file closes automatically.\n\n```python\n# Safe even with errors\nwith open('data.txt', 'r') as file:\n    data = file.read()\n    number = int(data)  # Might crash\n# File closes anyway, even if int() fails!\n```\n\n**Golden rule:** Always use `with` for file operations.\n\n### Topic 3: Reading Files\n\nThere are three main ways to read files, each useful for different situations.\n\n#### Method 1: `read()` - Get Everything\n\nReads the entire file as one string.\n\n```python\nwith open('story.txt', 'r') as file:\n    content = file.read()  # Returns one big string\n    print(content)\n    print(type(content))  # <class 'str'>\n\n# File contents: \"Once upon a time...\\nThere was a kingdom...\"\n# Result: 'Once upon a time...\\nThere was a kingdom...'\n```\n\n**When to use:** Small files, when you need everything at once.\n\n**Common mistake:** Using for huge files - loads entire file in memory!\n\n#### Method 2: `readlines()` - Get Line List\n\nReads entire file as a list, one item per line.\n\n```python\nwith open('shopping_list.txt', 'r') as file:\n    lines = file.readlines()  # Returns a list\n    print(lines)\n\n# File contents:\n# apples\n# bananas\n# carrots\n\n# Result: ['apples\\n', 'bananas\\n', 'carrots\\n']\nprint(type(lines))  # <class 'list'>\nprint(lines[0])    # 'apples\\n' (note: includes newline)\n```\n\n**When to use:** When you need to process each line separately.\n\n**Note:** Each line includes the `\\n` character. Strip it when needed:\n\n```python\nwith open('shopping_list.txt', 'r') as file:\n    lines = file.readlines()\n    # Remove newlines\n    clean_lines = [line.strip() for line in lines]\n    print(clean_lines)  # ['apples', 'bananas', 'carrots']\n```\n\n#### Method 3: `readline()` - Get One Line\n\nReads one line at a time. Useful for large files.\n\n```python\nwith open('log.txt', 'r') as file:\n    first_line = file.readline()   # Gets first line\n    second_line = file.readline()  # Gets second line\n    third_line = file.readline()   # Gets third line\n    print(first_line)    # Includes \\n\n    # Continue reading\n    for line in file:    # Iterating reads remaining lines\n        print(line.strip())\n```\n\n**When to use:** Processing huge files line-by-line without loading everything into memory.\n\n#### The Best Pattern: Loop Over File Object\n\n```python\n# This is the Pythonic way to read line-by-line\nwith open('data.txt', 'r') as file:\n    for line in file:  # Automatically reads one line per iteration\n        print(line.strip())  # Remove newline characters\n\n# Memory efficient even for massive files!\n```\n\n### Topic 4: Writing Files\n\nWriting puts data INTO files.\n\n#### Method 1: `write()` - Write Strings\n\n```python\nwith open('greeting.txt', 'w') as file:\n    file.write('Hello, World!')  # Writes string\n    file.write('\\n')             # Note: doesn't add newlines automatically\n    file.write('How are you?')\n\n# File now contains:\n# Hello, World!\n# How are you?\n```\n\n**Important:** `write()` doesn't add newlines. You must add `\\n` manually.\n\n```python\n# Common mistake\nwith open('output.txt', 'w') as file:\n    file.write('Line 1')\n    file.write('Line 2')\n# Result: \"Line 1Line 2\" (no line break!)\n\n# Fix: add newlines\nwith open('output.txt', 'w') as file:\n    file.write('Line 1\\n')\n    file.write('Line 2\\n')\n# Result: \"Line 1\\nLine 2\"\n```\n\n#### Method 2: `writelines()` - Write List of Strings\n\n```python\nwith open('grocery.txt', 'w') as file:\n    items = ['apples\\n', 'bananas\\n', 'carrots\\n']\n    file.writelines(items)  # Writes all items\n\n# File contains:\n# apples\n# bananas\n# carrots\n```\n\n**Note:** You must include `\\n` in your strings! `writelines()` doesn't add them.\n\n#### Practical Example: Collecting User Input\n\n```python\nwith open('responses.txt', 'w') as file:\n    for i in range(3):\n        response = input(f'Enter response {i+1}: ')\n        file.write(response + '\\n')  # Write with newline\n\nprint('Responses saved!')\n```\n\n### Topic 5: Appending to Files\n\nAppending ADDS to the end without erasing existing content.\n\n```python\n# Write initial content\nwith open('log.txt', 'w') as file:\n    file.write('=== Log Start ===\\n')\n\n# Later, append more entries\nwith open('log.txt', 'a') as file:\n    file.write('Event 1: User logged in\\n')\n    file.write('Event 2: File saved\\n')\n\n# File now contains:\n# === Log Start ===\n# Event 1: User logged in\n# Event 2: File saved\n```\n\n### Topic 6: File Positioning\n\nFile objects track their position (like a bookmark in a book).\n\n#### `tell()` - Where Are You?\n\n```python\nwith open('example.txt', 'r') as file:\n    print(file.tell())      # 0 (at start)\n    file.read(5)            # Read 5 characters\n    print(file.tell())      # 5 (now at position 5)\n    file.read()             # Read rest\n    print(file.tell())      # (at end)\n```\n\n#### `seek()` - Jump to Position\n\n```python\nwith open('example.txt', 'r') as file:\n    file.read(5)            # Read 5 characters\n    file.seek(0)            # Jump back to start\n    content = file.read()   # Read everything from beginning\n    print(content)\n```\n\n**Common use:** Rereading file content.\n\n### Topic 7: Text Encoding\n\nText files use **encodings** to store characters. Always specify:\n\n```python\n# CORRECT - explicitly set encoding\nwith open('data.txt', 'r', encoding='utf-8') as file:\n    data = file.read()\n\n# RISKY - relies on system default (might break on different computer!)\nwith open('data.txt', 'r') as file:\n    data = file.read()\n```\n\n**Why this matters:** Different encodings handle special characters differently. UTF-8 works worldwide.\n\n```python\n# UTF-8 handles international characters\nwith open('names.txt', 'w', encoding='utf-8') as file:\n    file.write('José')      # Spanish\n    file.write('Müller')    # German\n    file.write('李明')       # Chinese\n\n# Without encoding='utf-8', these might become garbage!\n```\n\n### Common Beginner Mistakes\n\n1. **Forgetting to close files**\n   ```python\n   # BAD\n   file = open('data.txt', 'r')\n   data = file.read()\n   # Missing close!\n   \n   # GOOD\n   with open('data.txt', 'r') as file:\n       data = file.read()\n   ```\n\n2. **Using 'w' when you meant 'a'**\n   ```python\n   # BAD - deletes existing content!\n   with open('log.txt', 'w') as file:\n       file.write('New entry')\n   \n   # GOOD - keeps existing content\n   with open('log.txt', 'a') as file:\n       file.write('New entry\\n')\n   ```\n\n3. **Forgetting newlines**\n   ```python\n   # BAD\n   file.write('Line 1')\n   file.write('Line 2')\n   # Result: \"Line 1Line 2\"\n   \n   # GOOD\n   file.write('Line 1\\n')\n   file.write('Line 2\\n')\n   ```\n\n4. **Not stripping newlines when reading**\n   ```python\n   # BAD\n   with open('data.txt', 'r') as file:\n       for line in file:\n           print(line)  # Prints with extra blank line\n   \n   # GOOD\n   with open('data.txt', 'r') as file:\n       for line in file:\n           print(line.strip())\n   ```\n\n### Quick Reference: Beginner Summary\n\n```python\n# Reading a file\nwith open('data.txt', 'r', encoding='utf-8') as file:\n    content = file.read()  # Entire file as string\n    # or\n    lines = file.readlines()  # List of lines\n\n# Writing to a file (creates or overwrites)\nwith open('output.txt', 'w', encoding='utf-8') as file:\n    file.write('Hello\\n')\n    file.write('World\\n')\n\n# Appending to a file (keeps existing content)\nwith open('log.txt', 'a', encoding='utf-8') as file:\n    file.write('New entry\\n')\n```",
  "intermediate": "## File I/O Operations for Intermediate Learners\n\n### Building on Basics\n\nNow that you understand file opening and closing, let's explore real-world patterns, error handling, and working with different file types.\n\n### Topic 1: Robust Error Handling\n\n#### Common File Exceptions\n\n```python\n# FileNotFoundError - file doesn't exist\ntry:\n    with open('nonexistent.txt', 'r') as file:\n        data = file.read()\nexcept FileNotFoundError:\n    print('Error: File not found')\n\n# PermissionError - can't read/write (permissions denied)\ntry:\n    with open('protected.txt', 'r') as file:\n        data = file.read()\nexcept PermissionError:\n    print('Error: You do not have permission to read this file')\n\n# IsADirectoryError - tried to open a directory as file\ntry:\n    with open('my_folder/', 'r') as file:\n        data = file.read()\nexcept IsADirectoryError:\n    print('Error: That is a directory, not a file')\n```\n\n#### Comprehensive Error Handling\n\n```python\ndef read_file_safely(filename):\n    \"\"\"Read file with proper error handling.\"\"\"\n    try:\n        with open(filename, 'r', encoding='utf-8') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f'Error: {filename} does not exist')\n        return None\n    except PermissionError:\n        print(f'Error: Permission denied reading {filename}')\n        return None\n    except UnicodeDecodeError:\n        print(f'Error: {filename} has encoding issues')\n        return None\n    except IOError as e:\n        print(f'Error reading file: {e}')\n        return None\n\n# Usage\ncontent = read_file_safely('data.txt')\nif content:\n    print(content)\n```\n\n#### Using OSError (Parent of FileNotFoundError)\n\n```python\n# OSError is parent class - catches multiple exceptions\ntry:\n    with open('data.txt', 'r') as file:\n        data = file.read()\nexcept OSError as e:\n    print(f'File operation failed: {e}')\n    # Handles FileNotFoundError, PermissionError, IsADirectoryError, etc.\n```\n\n### Topic 2: Working with Different File Types\n\n#### CSV Files (Comma-Separated Values)\n\n**Reading CSV:**\n\n```python\nimport csv\n\n# Simple approach - read and parse manually\nwith open('data.csv', 'r', encoding='utf-8') as file:\n    for line in file:\n        fields = line.strip().split(',')\n        print(fields)\n\n# Better approach - use csv module\nwith open('data.csv', 'r', encoding='utf-8') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)  # row is a list\n        # Access columns: row[0], row[1], etc.\n\n# Best approach - use csv.DictReader for labeled columns\nwith open('students.csv', 'r', encoding='utf-8') as file:\n    reader = csv.DictReader(file)  # Uses first row as headers\n    for row in reader:\n        print(row['name'], row['grade'])  # Access by column name\n        # row is a dictionary\n```\n\n**CSV file example:**\n```\nname,age,city\nAlice,25,New York\nBob,30,London\nCarol,28,Paris\n```\n\n**Writing CSV:**\n\n```python\nimport csv\n\nstudents = [\n    {'name': 'Alice', 'grade': 'A'},\n    {'name': 'Bob', 'grade': 'B'},\n    {'name': 'Carol', 'grade': 'A'},\n]\n\nwith open('grades.csv', 'w', newline='', encoding='utf-8') as file:\n    writer = csv.DictWriter(file, fieldnames=['name', 'grade'])\n    writer.writeheader()  # Write column names\n    writer.writerows(students)  # Write data\n\n# Result:\n# name,grade\n# Alice,A\n# Bob,B\n# Carol,A\n```\n\n**Why `newline=''`?** CSV module needs it to handle line endings correctly across platforms.\n\n#### JSON Files (JavaScript Object Notation)\n\n**Reading JSON:**\n\n```python\nimport json\n\n# JSON file contains: {\"name\": \"Alice\", \"age\": 25}\nwith open('person.json', 'r', encoding='utf-8') as file:\n    data = json.load(file)  # Parse JSON into Python dict\n    print(data['name'])  # Alice\n    print(data['age'])   # 25\n    print(type(data))    # <class 'dict'>\n\n# List of objects\nwith open('people.json', 'r', encoding='utf-8') as file:\n    people = json.load(file)\n    for person in people:\n        print(person['name'])\n```\n\n**JSON file example:**\n```json\n[\n  {\"name\": \"Alice\", \"age\": 25},\n  {\"name\": \"Bob\", \"age\": 30}\n]\n```\n\n**Writing JSON:**\n\n```python\nimport json\n\ndata = {\n    'name': 'Alice',\n    'age': 25,\n    'hobbies': ['reading', 'coding', 'hiking']\n}\n\n# Write as JSON\nwith open('data.json', 'w', encoding='utf-8') as file:\n    json.dump(data, file, indent=2)  # indent for pretty printing\n\n# Result:\n# {\n#   \"name\": \"Alice\",\n#   \"age\": 25,\n#   \"hobbies\": [\n#     \"reading\",\n#     \"coding\",\n#     \"hiking\"\n#   ]\n# }\n```\n\n### Topic 3: Binary Files\n\nBinary files store data in raw bytes (images, audio, compiled programs).\n\n#### Reading Binary Files\n\n```python\n# Read in binary mode ('rb')\nwith open('image.png', 'rb') as file:\n    binary_data = file.read()  # Returns bytes, not string\n    print(type(binary_data))   # <class 'bytes'>\n    print(binary_data[:10])    # First 10 bytes\n\n# Check first few bytes (file signature)\nwith open('image.png', 'rb') as file:\n    header = file.read(8)\n    if header.startswith(b'\\x89PNG'):  # PNG magic number\n        print('This is a PNG file')\n```\n\n#### Writing Binary Files\n\n```python\n# Copy a binary file\nwith open('original.jpg', 'rb') as source:\n    data = source.read()\n\nwith open('copy.jpg', 'wb') as destination:\n    destination.write(data)\n```\n\n#### Binary File Modes\n\n| Mode | Purpose |\n|------|----------|\n| `'rb'` | Read binary |\n| `'wb'` | Write binary (overwrites) |\n| `'ab'` | Append binary |\n| `'rb+'` | Read and write binary |\n\n**Why this matters:** Never try reading binary files with text mode - you'll get garbage:\n\n```python\n# WRONG - binary file in text mode\nwith open('image.png', 'r') as file:\n    data = file.read()  # Might fail or corrupt data\n\n# RIGHT\nwith open('image.png', 'rb') as file:\n    data = file.read()  # Correct\n```\n\n### Topic 4: Path Operations\n\n#### Using `pathlib` (Modern, Recommended)\n\n```python\nfrom pathlib import Path\n\n# Create path object\npath = Path('data/users.txt')\n\n# Check if file exists\nif path.exists():\n    print('File exists')\n\n# Get file information\nprint(path.name)           # 'users.txt'\nprint(path.stem)           # 'users' (without extension)\nprint(path.suffix)         # '.txt' (just extension)\nprint(path.parent)         # PosixPath('data')\nprint(path.absolute())     # Full absolute path\nprint(path.is_file())      # True if file\nprint(path.is_dir())       # True if directory\n\n# Get file size\nprint(path.stat().st_size)  # Size in bytes\n```\n\n#### Working with Paths\n\n```python\nfrom pathlib import Path\n\n# Build paths safely (works across Windows/Mac/Linux)\nbase = Path('my_project')\ndata_dir = base / 'data'  # Concatenate paths\nfile_path = data_dir / 'users.csv'\nprint(file_path)  # my_project/data/users.csv\n\n# Create directories\ndata_dir.mkdir(parents=True, exist_ok=True)\n# parents=True: create parent directories if needed\n# exist_ok=True: don't error if already exists\n\n# Read and write files with pathlib\nfile_path.write_text('Hello, World!')  # Write text\ncontent = file_path.read_text()        # Read text\nprint(content)  # Hello, World!\n\n# Binary files\nfile_path.write_bytes(b'\\x00\\x01\\x02')  # Write binary\ndata = file_path.read_bytes()            # Read binary\n```\n\n#### Listing Files\n\n```python\nfrom pathlib import Path\n\nfolder = Path('my_data')\n\n# List all items\nfor item in folder.iterdir():\n    print(item)\n\n# List only .txt files\nfor txt_file in folder.glob('*.txt'):\n    print(txt_file)\n\n# Recursive: all .py files in subfolders\nfor py_file in folder.glob('**/*.py'):\n    print(py_file)\n```\n\n#### `pathlib` vs `os.path` (Old Style)\n\n```python\nimport os\n\n# Old way - harder to read, platform-specific\npath = os.path.join('data', 'users.txt')\nif os.path.exists(path):\n    size = os.path.getsize(path)\n\n# New way - cleaner, more Pythonic\nfrom pathlib import Path\npath = Path('data') / 'users.txt'\nif path.exists():\n    size = path.stat().st_size\n```\n\n**Why pathlib is better:**\n- Cleaner syntax (use `/` instead of `os.path.join()`)\n- Returns objects with useful methods\n- Handles Windows/Mac/Linux automatically\n- More readable\n\n### Topic 5: Real-World Pattern: Safe File Operations\n\n#### Pattern 1: Atomic Write (Write Then Move)\n\nEnsure a file isn't corrupted if your program crashes:\n\n```python\nimport tempfile\nfrom pathlib import Path\n\ndef safe_write(filename, content):\n    \"\"\"Write file atomically - either all succeeds or all fails.\"\"\"\n    path = Path(filename)\n    \n    # Write to temporary file first\n    with tempfile.NamedTemporaryFile(mode='w', delete=False, \n                                     dir=path.parent, encoding='utf-8') as tmp:\n        tmp.write(content)\n        tmp_path = Path(tmp.name)\n    \n    # Move temp file to real location (atomic operation)\n    tmp_path.replace(path)\n\n# Usage\nsafe_write('important.txt', 'Critical data')\n```\n\nWhy this matters: If your program crashes while writing, the original file is untouched.\n\n#### Pattern 2: Read and Modify\n\n```python\ndef modify_file(filename, modify_func):\n    \"\"\"Read file, apply function to content, write back.\"\"\"\n    with open(filename, 'r', encoding='utf-8') as file:\n        content = file.read()\n    \n    modified = modify_func(content)\n    \n    with open(filename, 'w', encoding='utf-8') as file:\n        file.write(modified)\n\n# Usage\ndef remove_trailing_spaces(text):\n    return '\\n'.join(line.rstrip() for line in text.split('\\n'))\n\nmodify_file('data.txt', remove_trailing_spaces)\n```\n\n#### Pattern 3: Process Large Files Line-by-Line\n\n```python\ndef process_large_file(input_file, output_file, process_func):\n    \"\"\"Process huge files without loading all into memory.\"\"\"\n    with open(input_file, 'r', encoding='utf-8') as infile, \\\n          open(output_file, 'w', encoding='utf-8') as outfile:\n        for line in infile:\n            processed = process_func(line.strip())\n            outfile.write(processed + '\\n')\n\n# Usage: convert all words to uppercase\nprocess_large_file('input.txt', 'output.txt', str.upper)\n```\n\n#### Pattern 4: Context Manager for Multiple Files\n\n```python\n# Open multiple files at once\nwith open('input.txt', 'r', encoding='utf-8') as infile, \\\n      open('output.txt', 'w', encoding='utf-8') as outfile:\n    for line in infile:\n        processed = line.strip().upper()\n        outfile.write(processed + '\\n')\n# Both files close automatically\n```\n\n### Topic 6: File Modes Reference Table\n\n| Mode | Opens Existing | Creates New | Reads | Writes | Overwrites |\n|------|---|---|---|---|---|\n| `'r'` | ✓ | ✗ | ✓ | ✗ | N/A |\n| `'w'` | ✓ | ✓ | ✗ | ✓ | ✓ |\n| `'a'` | ✓ | ✓ | ✗ | ✓ | ✗ |\n| `'x'` | ✗ | ✓ | ✗ | ✓ | N/A |\n| `'r+'` | ✓ | ✗ | ✓ | ✓ | ✗ |\n| `'w+'` | ✓ | ✓ | ✓ | ✓ | ✓ |\n\n### Topic 7: Working with File Encodings\n\n#### Handling Encoding Errors\n\n```python\n# Different error handling strategies\n\n# 1. Strict (default) - raise error\ntry:\n    with open('text.txt', 'r', encoding='utf-8') as file:\n        data = file.read()\nexcept UnicodeDecodeError:\n    print('File has encoding errors')\n\n# 2. Replace bad characters\nwith open('text.txt', 'r', encoding='utf-8', errors='replace') as file:\n    data = file.read()  # Bad chars become '?'\n\n# 3. Ignore bad characters\nwith open('text.txt', 'r', encoding='utf-8', errors='ignore') as file:\n    data = file.read()  # Bad chars are skipped\n\n# 4. Substitute with placeholder\nwith open('text.txt', 'r', encoding='utf-8', errors='backslashreplace') as file:\n    data = file.read()  # Bad chars become \\xXX\n```\n\n#### Detecting Encoding\n\n```python\n# Use chardet library for auto-detection\nimport chardet\n\nwith open('mystery.txt', 'rb') as file:\n    raw_data = file.read()\n    result = chardet.detect(raw_data)\n    encoding = result['encoding']\n    confidence = result['confidence']\n    print(f'Detected: {encoding} (confidence: {confidence})')\n\n# Read with detected encoding\nwith open('mystery.txt', 'r', encoding=encoding) as file:\n    data = file.read()\n```\n\n### Common Intermediate Mistakes\n\n1. **Not handling exceptions**\n   ```python\n   # BAD\n   content = open('file.txt').read()\n   \n   # GOOD\n   try:\n       with open('file.txt', 'r', encoding='utf-8') as file:\n           content = file.read()\n   except FileNotFoundError:\n       print('File not found')\n       content = None\n   ```\n\n2. **Using 'r' mode for binary files**\n   ```python\n   # BAD\n   with open('image.png', 'r') as file:\n       data = file.read()\n   \n   # GOOD\n   with open('image.png', 'rb') as file:\n       data = file.read()\n   ```\n\n3. **Not setting encoding explicitly**\n   ```python\n   # RISKY - uses system default\n   with open('text.txt', 'r') as file:\n       data = file.read()\n   \n   # SAFE\n   with open('text.txt', 'r', encoding='utf-8') as file:\n       data = file.read()\n   ```\n\n4. **Using string methods on binary data**\n   ```python\n   # BAD\n   with open('file.bin', 'rb') as file:\n       data = file.read()\n       text = data.decode()  # Wrong!\n   \n   # GOOD\n   with open('file.bin', 'rb') as file:\n       data = file.read()\n       text = data.decode('utf-8')  # Specify encoding\n   ```",
  "advanced": "## File I/O Operations for Advanced Learners\n\n### Deep Dive into File I/O Internals\n\n### Topic 1: Understanding Buffering\n\n#### How Buffering Works\n\nFiles use buffers to optimize performance. Instead of writing immediately to disk, data sits in memory and gets written in batches.\n\n```python\n# Buffering happens automatically\nwith open('large_output.txt', 'w') as file:\n    for i in range(1000000):\n        file.write(f'Line {i}\\n')\n        # Data doesn't go to disk immediately!\n        # Python buffers it internally\n\n# When the context manager exits, all buffered data flushes to disk\n```\n\n#### Flushing Buffers Manually\n\n```python\nimport time\n\n# Force write to disk immediately (slow but safe)\nwith open('log.txt', 'a') as file:\n    for event in events:\n        file.write(f'{event}\\n')\n        file.flush()  # Write immediately to disk\n        time.sleep(0.1)\n\n# Why this matters: if power cuts out after flush(), data is safe on disk\n```\n\n#### Buffer Sizes\n\n```python\n# Open with specific buffer size (in bytes)\nwith open('data.txt', 'w', buffering=8192) as file:  # 8KB buffer\n    file.write('data')  # Held in 8KB buffer\n\n# Unbuffered (immediate disk writes - very slow)\nwith open('data.txt', 'w', buffering=0) as file:\n    file.write('data')  # Goes to disk immediately\n\n# Line buffered (write on newline)\nwith open('log.txt', 'w', buffering=1) as file:\n    file.write('Log entry\\n')  # Flushed to disk on newline\n    file.write('Partial')      # Not flushed yet\n\n# Full buffering (write in chunks)\nwith open('data.txt', 'w', buffering=-1) as file:  # -1 uses default\n    file.write('data')\n```\n\n**Performance impact:**\n\n```python\nimport time\n\n# Approach 1: Unbuffered (slow - 1 disk write per line)\nstart = time.time()\nwith open('output1.txt', 'w', buffering=0) as file:\n    for i in range(10000):\n        file.write(f'Line {i}\\n')\nprint(f'Unbuffered: {time.time() - start:.2f}s')  # ~5 seconds\n\n# Approach 2: Default buffering (fast - batched disk writes)\nstart = time.time()\nwith open('output2.txt', 'w') as file:\n    for i in range(10000):\n        file.write(f'Line {i}\\n')\nprint(f'Default buffering: {time.time() - start:.2f}s')  # ~0.1 seconds\n\n# Approach 3: Batch writes (fastest)\nstart = time.time()\nwith open('output3.txt', 'w') as file:\n    lines = [f'Line {i}\\n' for i in range(10000)]\n    file.writelines(lines)\nprint(f'Batch write: {time.time() - start:.2f}s')  # ~0.05 seconds\n```\n\n### Topic 2: Reading Large Files Efficiently\n\n#### The Memory Problem\n\n```python\n# Problem: reading 5GB file at once\nwith open('huge_file.txt', 'r') as file:\n    data = file.read()  # ALL 5GB loaded into RAM at once!\n    # If you only have 8GB RAM, computer might crash\n```\n\n#### Solution 1: Read in Chunks\n\n```python\ndef read_in_chunks(filename, chunk_size=8192):\n    \"\"\"Read file in small chunks.\"\"\"\n    with open(filename, 'rb') as file:\n        while True:\n            chunk = file.read(chunk_size)  # Read 8KB at a time\n            if not chunk:\n                break\n            yield chunk  # Process chunk without holding all data\n\n# Usage: process huge file\ntotal_size = 0\nfor chunk in read_in_chunks('huge_file.bin'):\n    total_size += len(chunk)\n    # Process chunk, then it's discarded\n\nprint(f'Total size: {total_size} bytes')\n```\n\n#### Solution 2: Line-by-Line Iteration\n\n```python\n# Most Pythonic way - automatic buffering\nwith open('huge_file.txt', 'r') as file:\n    for line in file:  # One line at a time in memory\n        processed = process_line(line)\n        # Previous line is garbage collected\n\n# Behind the scenes:\n# - File object reads 8KB buffer internally\n# - Yields one line at a time\n# - Automatically refills buffer when empty\n```\n\n#### Solution 3: Memory Mapping (Advanced)\n\n```python\nimport mmap\n\n# Memory map: file appears as giant string in memory\nwith open('large_file.txt', 'rb') as file:\n    with mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ) as mmapped:\n        # File appears as bytes object\n        # Only sections you access are loaded into RAM\n        \n        # Find a string\n        index = mmapped.find(b'search_term')\n        if index != -1:\n            print(f'Found at byte {index}')\n        \n        # Access specific sections\n        section = mmapped[1000:2000]  # Only these bytes loaded\n```\n\n**Why mmap matters:** For very large files, only accessed portions load into RAM.\n\n### Topic 3: Advanced Binary File Handling\n\n#### Reading Structured Binary Data\n\n```python\nimport struct\n\n# Binary format: 4-byte int, 4-byte float, 2-byte short\ndef read_records(filename):\n    \"\"\"Read binary records from file.\"\"\"\n    with open(filename, 'rb') as file:\n        while True:\n            # Read 10 bytes (4 + 4 + 2)\n            data = file.read(10)\n            if len(data) < 10:\n                break\n            \n            # Unpack binary data\n            value_int, value_float, value_short = struct.unpack('<ifh', data)\n            # '<' = little-endian, 'i' = int, 'f' = float, 'h' = short\n            \n            yield {\n                'int': value_int,\n                'float': value_float,\n                'short': value_short\n            }\n\n# Usage\nfor record in read_records('data.bin'):\n    print(record)\n```\n\n#### Writing Structured Binary Data\n\n```python\nimport struct\n\ndef write_records(filename, records):\n    \"\"\"Write structured binary data.\"\"\"\n    with open(filename, 'wb') as file:\n        for record in records:\n            # Pack data into binary\n            binary = struct.pack('<ifh',\n                record['int'],\n                record['float'],\n                record['short']\n            )\n            file.write(binary)\n\n# Usage\ndata = [\n    {'int': 42, 'float': 3.14, 'short': 100},\n    {'int': 99, 'float': 2.71, 'short': 200},\n]\nwrite_records('data.bin', data)\n```\n\n#### Reading With Seek and Tell\n\n```python\ndef read_middle_of_file(filename):\n    \"\"\"Jump to middle of file without reading everything.\"\"\"\n    with open(filename, 'rb') as file:\n        # Get file size\n        file.seek(0, 2)  # Seek to end (0 bytes from end)\n        file_size = file.tell()\n        \n        # Jump to middle\n        file.seek(file_size // 2)\n        \n        # Read 100 bytes from middle\n        data = file.read(100)\n        return data\n\n# Much faster than reading whole file\nmiddle_data = read_middle_of_file('huge_file.bin')\n```\n\n#### Seek Constants\n\n```python\nimport os\n\nwith open('data.txt', 'rb') as file:\n    # Seek modes\n    file.seek(100, os.SEEK_SET)    # 100 bytes from start (default)\n    file.seek(-50, os.SEEK_END)    # 50 bytes from end\n    file.seek(10, os.SEEK_CUR)     # 10 bytes from current position\n```\n\n### Topic 4: Performance Optimization Strategies\n\n#### Strategy 1: Batch Operations\n\n```python\n# SLOW: Individual writes\nwith open('output.txt', 'w') as file:\n    for item in items:\n        file.write(item + '\\n')\n\n# FASTER: Batch write\nwith open('output.txt', 'w') as file:\n    batch = '\\n'.join(items) + '\\n'\n    file.write(batch)\n\n# Timing comparison\nimport time\n\nitems = [f'Item {i}' for i in range(100000)]\n\n# Method 1: Individual writes\nstart = time.time()\nwith open('test1.txt', 'w') as f:\n    for item in items:\n        f.write(item + '\\n')\nprint(f'Individual: {time.time() - start:.3f}s')\n\n# Method 2: Batch\nstart = time.time()\nwith open('test2.txt', 'w') as f:\n    f.write('\\n'.join(items) + '\\n')\nprint(f'Batch: {time.time() - start:.3f}s')\n```\n\n#### Strategy 2: Use Efficient Data Structures\n\n```python\n# Process CSV to JSON efficiently\nimport csv\nimport json\nfrom io import StringIO\n\ndef convert_csv_to_json(csv_file, json_file, chunk_size=1000):\n    \"\"\"Convert large CSV to JSON in chunks.\"\"\"\n    with open(json_file, 'w') as out:\n        out.write('[\\n')\n        first = True\n        \n        with open(csv_file, 'r') as infile:\n            reader = csv.DictReader(infile)\n            for i, row in enumerate(reader):\n                if not first:\n                    out.write(',\\n')\n                json.dump(row, out)\n                first = False\n        \n        out.write('\\n]')\n```\n\n#### Strategy 3: Parallel Processing\n\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\n\ndef process_file(filepath):\n    \"\"\"Process a single file.\"\"\"\n    with open(filepath, 'r') as f:\n        content = f.read()\n    # Do expensive processing\n    return len(content)\n\ndef process_multiple_files(directory):\n    \"\"\"Process multiple files in parallel.\"\"\"\n    files = list(Path(directory).glob('*.txt'))\n    \n    with ThreadPoolExecutor(max_workers=4) as executor:\n        results = list(executor.map(process_file, files))\n    \n    return results\n\n# Much faster than sequential processing\nresults = process_multiple_files('data_folder')\n```\n\n### Topic 5: Context Managers Deep Dive\n\n#### How Context Managers Work\n\n```python\nclass FileManager:\n    \"\"\"Simple file context manager.\"\"\"\n    \n    def __init__(self, filename, mode):\n        self.filename = filename\n        self.mode = mode\n        self.file = None\n    \n    def __enter__(self):\n        \"\"\"Called when entering 'with' block.\"\"\"\n        print(f'Opening {self.filename}')\n        self.file = open(self.filename, self.mode)\n        return self.file\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Called when exiting 'with' block (even on error).\"\"\"\n        print(f'Closing {self.filename}')\n        if self.file:\n            self.file.close()\n        \n        # Return True to suppress exceptions\n        if exc_type is not None:\n            print(f'Error occurred: {exc_type.__name__}')\n        return False\n\n# Usage\nwith FileManager('data.txt', 'r') as file:\n    content = file.read()\n\n# Output:\n# Opening data.txt\n# Closing data.txt\n```\n\n#### Using `@contextmanager` Decorator\n\n```python\nfrom contextlib import contextmanager\nimport tempfile\nfrom pathlib import Path\n\n@contextmanager\ndef temporary_directory():\n    \"\"\"Context manager for temporary directories.\"\"\"\n    tmpdir = tempfile.mkdtemp()\n    try:\n        yield Path(tmpdir)\n    finally:\n        # Cleanup\n        import shutil\n        shutil.rmtree(tmpdir)\n\n# Usage\nwith temporary_directory() as tmpdir:\n    test_file = tmpdir / 'test.txt'\n    test_file.write_text('temporary data')\n    print(test_file.read_text())\n# tmpdir automatically deleted\n```\n\n#### Context Manager for Exception Handling\n\n```python\nfrom contextlib import contextmanager\n\n@contextmanager\ndef file_transaction(filename):\n    \"\"\"Context manager for atomic file operations.\"\"\"\n    import tempfile\n    from pathlib import Path\n    \n    path = Path(filename)\n    \n    # Create temp file\n    with tempfile.NamedTemporaryFile(mode='w', delete=False, \n                                     dir=path.parent) as tmp:\n        tmp_path = Path(tmp.name)\n        try:\n            yield tmp\n        except Exception:\n            # On error, delete temp file\n            tmp_path.unlink()\n            raise\n        else:\n            # On success, replace original\n            tmp_path.replace(path)\n\n# Usage\nwith file_transaction('important.txt') as f:\n    f.write('Critical data')\n    # If exception occurs, original file untouched\n```\n\n### Topic 6: Advanced Path Handling\n\n#### Working with File Metadata\n\n```python\nfrom pathlib import Path\nimport os\nfrom datetime import datetime\n\npath = Path('document.txt')\n\n# File information\nstats = path.stat()\nprint(f'Size: {stats.st_size} bytes')\nprint(f'Created: {datetime.fromtimestamp(stats.st_ctime)}')\nprint(f'Modified: {datetime.fromtimestamp(stats.st_mtime)}')\nprint(f'Accessed: {datetime.fromtimestamp(stats.st_atime)}')\nprint(f'Permissions: {oct(stats.st_mode)}')\n\n# Owner information (Unix-like systems)\nprint(f'Owner UID: {stats.st_uid}')\nprint(f'Owner GID: {stats.st_gid}')\n```\n\n#### Recursive Directory Operations\n\n```python\nfrom pathlib import Path\nimport os\n\ndef find_and_process(directory, pattern, processor):\n    \"\"\"Find all matching files and process them.\"\"\"\n    path = Path(directory)\n    \n    # Recursive glob\n    for file in path.rglob(pattern):  # ** is recursive\n        if file.is_file():\n            processor(file)\n\ndef sum_file_sizes(directory):\n    \"\"\"Calculate total size of all files recursively.\"\"\"\n    total = 0\n    for file in Path(directory).rglob('*'):\n        if file.is_file():\n            total += file.stat().st_size\n    return total\n\n# Usage\ntotal_size = sum_file_sizes('my_project')\nprint(f'Total size: {total_size / 1024 / 1024:.2f} MB')\n```\n\n### Topic 7: Handling Errors at OS Level\n\n#### Race Conditions\n\n```python\nfrom pathlib import Path\nimport os\n\n# RISKY: File might be deleted between check and open\nif Path('temp.txt').exists():\n    with open('temp.txt', 'r') as f:  # Could fail here!\n        data = f.read()\n\n# SAFE: Ask forgiveness, not permission (EAFP)\ntry:\n    with open('temp.txt', 'r') as f:\n        data = f.read()\nexcept FileNotFoundError:\n    data = None\n```\n\n#### Atomic File Operations\n\n```python\nfrom pathlib import Path\nimport os\n\ndef atomic_write(filename, content):\n    \"\"\"Write file atomically.\"\"\"\n    path = Path(filename)\n    \n    # Write to temp file (same filesystem)\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', dir=path.parent, \n                                     delete=False) as tmp:\n        tmp.write(content)\n        tmp_path = tmp.name\n    \n    # Atomic rename (all or nothing)\n    os.replace(tmp_path, path)  # Atomic on all major OS\n\n# Usage\natomic_write('config.txt', '{\"setting\": true}')\n```\n\n#### Locking Files\n\n```python\nimport fcntl  # Unix/Linux\nimport msvcrt  # Windows\n\ndef exclusive_write(filename, content):\n    \"\"\"Write with exclusive file lock.\"\"\"\n    with open(filename, 'w') as f:\n        # Lock file on Unix\n        fcntl.flock(f.fileno(), fcntl.LOCK_EX)  # Exclusive lock\n        try:\n            f.write(content)\n        finally:\n            fcntl.flock(f.fileno(), fcntl.LOCK_UN)  # Unlock\n\n# Only one process can hold lock at a time\n```\n\n### Topic 8: Compression and Archiving\n\n#### Reading Compressed Files\n\n```python\nimport gzip\nimport bz2\n\n# GZIP\nwith gzip.open('data.txt.gz', 'rt', encoding='utf-8') as f:\n    content = f.read()  # Automatically decompressed\n\n# BZ2\nwith bz2.open('data.txt.bz2', 'rt', encoding='utf-8') as f:\n    content = f.read()\n```\n\n#### Writing Compressed Files\n\n```python\nimport gzip\n\nwith gzip.open('output.txt.gz', 'wt', encoding='utf-8', compresslevel=9) as f:\n    f.write('Large amount of data')\n    # compresslevel: 1 (fast) to 9 (best compression)\n```\n\n#### Working with ZIP Archives\n\n```python\nimport zipfile\nfrom pathlib import Path\n\n# Read from ZIP\nwith zipfile.ZipFile('archive.zip', 'r') as zf:\n    # List contents\n    for name in zf.namelist():\n        print(name)\n    \n    # Read a file from ZIP\n    with zf.open('file.txt') as f:\n        content = f.read()\n\n# Create ZIP\nwith zipfile.ZipFile('archive.zip', 'w') as zf:\n    zf.write('file1.txt')  # Add file\n    zf.writestr('file2.txt', 'content here')  # Write from string\n```\n\n### Performance Benchmarking\n\n```python\nimport timeit\nfrom pathlib import Path\n\n# Create test file\nPath('test.txt').write_text('line\\n' * 100000)\n\n# Benchmark different read methods\nsetup = \"from pathlib import Path\"\n\n# Method 1: read()\nt1 = timeit.timeit(\n    \"Path('test.txt').read_text()\",\n    setup=setup, number=100\n)\n\n# Method 2: readlines()\nt2 = timeit.timeit(\n    \"Path('test.txt').read_text().split('\\\\n')\",\n    setup=setup, number=100\n)\n\n# Method 3: line iteration\nt3 = timeit.timeit(\n    \"[line for line in open('test.txt')]\",\n    setup=setup, number=100\n)\n\nprint(f'read_text(): {t1:.3f}s')\nprint(f'split():     {t2:.3f}s')\nprint(f'iteration:   {t3:.3f}s')\n```",
  "cheatsheet": "## File I/O Cheatsheet - Quick Reference\n\n### Opening Files\n\n```python\nfrom pathlib import Path\n\n# Standard way (BEST PRACTICE)\nwith open('file.txt', 'r', encoding='utf-8') as f:\n    content = f.read()\n\n# Using pathlib (Modern)\npath = Path('file.txt')\ncontent = path.read_text(encoding='utf-8')\n\n# Multiple files\nwith open('input.txt', 'r') as infile, open('output.txt', 'w') as outfile:\n    outfile.write(infile.read().upper())\n```\n\n### File Modes at a Glance\n\n```python\n'r'    # Read (file must exist)\n'w'    # Write (creates/overwrites)\n'a'    # Append (adds to end)\n'x'    # Exclusive create (error if exists)\n'b'    # Binary (add to above: 'rb', 'wb', etc.)\n'+'    # Read+Write (add to above: 'r+', 'w+', etc.)\n\n# Examples:\nopen('file.txt', 'r')    # Text read\nopen('data.bin', 'rb')   # Binary read\nopen('file.txt', 'w')    # Text write\nopen('log.txt', 'a')     # Text append\n```\n\n### Reading Files\n\n```python\n# Read entire file\nwith open('file.txt', 'r') as f:\n    content = f.read()  # Returns string\n\n# Read all lines as list\nwith open('file.txt', 'r') as f:\n    lines = f.readlines()  # ['line1\\n', 'line2\\n']\n\n# Read one line\nwith open('file.txt', 'r') as f:\n    line = f.readline()  # 'line1\\n'\n\n# Read line by line (BEST for large files)\nwith open('file.txt', 'r') as f:\n    for line in f:\n        print(line.strip())  # Remove newline\n\n# Read into list\nwith open('file.txt', 'r') as f:\n    lines = [line.rstrip('\\n') for line in f]\n```\n\n### Writing Files\n\n```python\n# Write string\nwith open('file.txt', 'w') as f:\n    f.write('Hello\\n')\n    f.write('World\\n')\n\n# Write multiple strings\nwith open('file.txt', 'w') as f:\n    f.writelines(['Hello\\n', 'World\\n'])\n\n# Append to file\nwith open('file.txt', 'a') as f:\n    f.write('New line\\n')\n\n# Write list of items\nwith open('file.txt', 'w') as f:\n    f.write('\\n'.join(items) + '\\n')\n```\n\n### Binary Files\n\n```python\n# Read binary\nwith open('image.png', 'rb') as f:\n    data = f.read()  # Returns bytes\n\n# Write binary\nwith open('output.bin', 'wb') as f:\n    f.write(b'\\x00\\x01\\x02')\n\n# Copy binary file\nwith open('original.jpg', 'rb') as src, open('copy.jpg', 'wb') as dst:\n    dst.write(src.read())\n\n# Read binary in chunks\nwith open('huge.bin', 'rb') as f:\n    while True:\n        chunk = f.read(8192)  # 8KB chunks\n        if not chunk:\n            break\n        process(chunk)\n```\n\n### File Positioning\n\n```python\nwith open('file.txt', 'r') as f:\n    print(f.tell())      # Current position\n    f.read(10)           # Read 10 chars\n    print(f.tell())      # Position is now 10\n    f.seek(0)            # Jump to start\n    content = f.read()   # Read from beginning\n    \n    # Seek modes\n    f.seek(0, 0)        # From start (default)\n    f.seek(-10, 2)      # 10 bytes from end\n    f.seek(5, 1)        # 5 bytes from current\n```\n\n### Path Operations\n\n```python\nfrom pathlib import Path\n\npath = Path('data/file.txt')\n\n# File checks\npath.exists()          # True if exists\npath.is_file()         # True if file\npath.is_dir()          # True if directory\n\n# Path info\npath.name              # 'file.txt'\npath.stem              # 'file'\npath.suffix            # '.txt'\npath.parent            # Path('data')\npath.absolute()        # Full path\npath.stat().st_size    # File size in bytes\n\n# Path construction\npath = Path('data') / 'subfolder' / 'file.txt'\n\n# Create directories\nPath('data/subfolder').mkdir(parents=True, exist_ok=True)\n\n# List files\nfor file in Path('data').glob('*.txt'):  # All .txt in data\n    print(file)\n\nfor file in Path('data').rglob('*.py'):  # All .py recursively\n    print(file)\n```\n\n### CSV Operations\n\n```python\nimport csv\n\n# Read CSV\nwith open('data.csv', 'r') as f:\n    reader = csv.DictReader(f)  # Use headers as keys\n    for row in reader:\n        print(row['name'], row['age'])\n\n# Write CSV\nwith open('output.csv', 'w', newline='') as f:\n    writer = csv.DictWriter(f, fieldnames=['name', 'age'])\n    writer.writeheader()\n    writer.writerow({'name': 'Alice', 'age': 25})\n\n# Simple reader (returns lists)\nwith open('data.csv', 'r') as f:\n    reader = csv.reader(f)\n    for row in reader:\n        print(row[0], row[1])\n```\n\n### JSON Operations\n\n```python\nimport json\n\n# Read JSON\nwith open('data.json', 'r') as f:\n    data = json.load(f)  # Returns dict or list\n\n# Write JSON\nwith open('output.json', 'w') as f:\n    json.dump(data, f, indent=2)  # Pretty print\n\n# JSON from string\ndata = json.loads('{\"name\": \"Alice\"}')\njson_str = json.dumps(data)\n```\n\n### Error Handling\n\n```python\n# Basic try-except\ntry:\n    with open('file.txt', 'r') as f:\n        data = f.read()\nexcept FileNotFoundError:\n    print('File not found')\nexcept PermissionError:\n    print('No permission')\nexcept IOError as e:\n    print(f'I/O error: {e}')\n\n# Get specific errors\ntry:\n    with open('file.txt', 'r') as f:\n        lines = f.readlines()\nexcept OSError as e:\n    print(f'Error: {e.strerror}')\n\n# Check before opening\nfrom pathlib import Path\n\nif Path('file.txt').exists():\n    with open('file.txt', 'r') as f:\n        data = f.read()\nelse:\n    print('File not found')\n```\n\n### Common Patterns\n\n```python\n# Safe write (atomic)\nimport tempfile\n\nwith tempfile.NamedTemporaryFile(mode='w', delete=False) as tmp:\n    tmp.write(content)\n    tmp_path = tmp.name\n\nimport os\nos.replace(tmp_path, 'final.txt')\n\n# Read and modify\nwith open('file.txt', 'r') as f:\n    content = f.read()\n\nmodified = content.replace('old', 'new')\n\nwith open('file.txt', 'w') as f:\n    f.write(modified)\n\n# Process large file\nwith open('huge.txt', 'r') as infile, open('output.txt', 'w') as outfile:\n    for line in infile:\n        processed = line.strip().upper()\n        outfile.write(processed + '\\n')\n\n# Count lines\nwith open('file.txt', 'r') as f:\n    line_count = sum(1 for _ in f)\n\n# Get file size\nfrom pathlib import Path\nsize_bytes = Path('file.txt').stat().st_size\nsize_mb = size_bytes / 1024 / 1024\n```\n\n### Encodings\n\n```python\n# Always specify encoding\nwith open('file.txt', 'r', encoding='utf-8') as f:\n    data = f.read()\n\n# Handle encoding errors\nwith open('file.txt', 'r', encoding='utf-8', errors='replace') as f:\n    data = f.read()  # Bad chars become '?'\n\n# Common encodings\n'utf-8'      # Unicode (recommended)\n'latin-1'    # Western European\n'ascii'      # Basic ASCII only\n'cp1252'     # Windows Western\n```\n\n### Performance Tips\n\n```python\n# SLOW: Individual operations\nfor item in items:\n    file.write(item + '\\n')\n\n# FAST: Batch operations\nfile.write('\\n'.join(items) + '\\n')\n\n# SLOW: Read entire huge file\nwith open('huge.txt', 'r') as f:\n    content = f.read()  # Loads all to RAM\n\n# FAST: Process line by line\nwith open('huge.txt', 'r') as f:\n    for line in f:  # Only current line in RAM\n        process(line)\n\n# FAST: Use pathlib for multiple operations\nfrom pathlib import Path\npath = Path('file.txt')\npath.write_text('data')  # One operation\ncontent = path.read_text()  # One operation\n```\n\n### Compression\n\n```python\nimport gzip\nimport bz2\n\n# Read gzip\nwith gzip.open('file.txt.gz', 'rt') as f:\n    content = f.read()\n\n# Write gzip\nwith gzip.open('output.txt.gz', 'wt') as f:\n    f.write('data')\n\n# BZ2\nwith bz2.open('file.bz2', 'rt') as f:\n    content = f.read()\n```\n\n### Context Manager Pattern\n\n```python\nfrom contextlib import contextmanager\n\n@contextmanager\ndef open_safely(filename):\n    f = open(filename, 'r')\n    try:\n        yield f\n    finally:\n        f.close()\n\n# Usage\nwith open_safely('file.txt') as f:\n    data = f.read()\n```\n\n### Debugging File Operations\n\n```python\n# Check file exists and readable\nfrom pathlib import Path\nimport os\n\npath = Path('file.txt')\nif not path.exists():\n    print('File not found')\nelif not os.access(path, os.R_OK):\n    print('File not readable')\nelse:\n    print(f'File size: {path.stat().st_size} bytes')\n    print(f'File mode: {oct(path.stat().st_mode)}')\n\n# List all open files (Linux)\nimport subprocess\nresult = subprocess.run(['lsof'], capture_output=True, text=True)\nprint(result.stdout)\n```\n\n### One-Liners\n\n```python\n# Read entire file\ncontent = open('file.txt').read()\n\n# Read into list\nlines = open('file.txt').readlines()\n\n# Count lines\ncount = sum(1 for _ in open('file.txt'))\n\n# Write string to file\nopen('file.txt', 'w').write('content')\n\n# Get file size\nimport os\nsize = os.path.getsize('file.txt')\n\n# Check if file exists\nimport os\nexists = os.path.exists('file.txt')\n\n# Read as bytes\ndata = open('file.bin', 'rb').read()\n```"
}
